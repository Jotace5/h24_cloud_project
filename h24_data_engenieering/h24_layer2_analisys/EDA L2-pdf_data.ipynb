{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration and Analysis (EDA) to Determine Relevant Files for Training Process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo csv\n",
    "df = pd.read_csv('metadata_2.0.csv')\n",
    "\n",
    "# Mostrar las primeras 5 filas\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of File Creators\n",
    "\n",
    "Creators of the files are categorized into numerical labels for further analysis.\n",
    "\n",
    "- **1 - Samsung-M4580FX**\n",
    "- **2 - intsig.com pdf producer**\n",
    "- **3 - 3-Heights™ PDF Merge Split Shell 6.12.1.11 (http://www.pdf-tools.com)**\n",
    "- **4 - Microsoft® Word 2019**\n",
    "- **5 - iLovePDF**\n",
    "- **6 - RxRelease / Haru2.4.0dev**\n",
    "- **7 - iText® 5.4.4 ©2000-2013 1T3XT BVBA (AGPL-version)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Path to the CSV file\n",
    "route = r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\ETF Layer2 - Pdf_datset\\metadata_2.0.csv'\n",
    "\n",
    "# Folder where the files are currently located and where the new producer folders will be created\n",
    "source_folder = r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\ETF Layer2 - Pdf_datset\\Training Dataset\\metadata_sample'\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(route)\n",
    "\n",
    "# For each row in the DataFrame\n",
    "for index, row in data.iterrows():\n",
    "    file_name = row['Name']\n",
    "    producer = row['Producer']\n",
    "\n",
    "    # Create a folder for the producer if it does not exist\n",
    "    producer_folder = os.path.join(source_folder, producer)\n",
    "    if not os.path.exists(producer_folder):\n",
    "        os.makedirs(producer_folder)\n",
    "\n",
    "    # Move the file to the producer's folder\n",
    "    source_file_path = os.path.join(source_folder, file_name)\n",
    "    dest_file_path = os.path.join(producer_folder, file_name)\n",
    "    \n",
    "    # Check if the file exists in the original location before moving it\n",
    "    if os.path.exists(source_file_path):\n",
    "        shutil.move(source_file_path, dest_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata folder is divided into 7 subfolders:\n",
    "\n",
    "- **Samsung-M4580FX**: This producer appears to be the most common in the dataset, suggesting that many of these documents were likely created or scanned using a Samsung M4580FX machine.\n",
    "- **intsig.com pdf producer**: Represents another significant portion of the documents. \"CamScanner\" is often associated with this as the creator, indicating that many of these documents were likely scanned using the CamScanner app.\n",
    "- **iLovePDF**: Appears several times, suggesting that this tool was used to modify or combine PDFs.\n",
    "- **3-Heights™ PDF Merge Split Shell**: This indicates the use of PDF-Tools to merge or split the documents.\n",
    "- **Microsoft® Word 2019**: Some documents were produced directly from MS Word.\n",
    "- **iText® 5.4.4**: An older version of iText was used for some PDFs, which is a library for creating and manipulating PDFs.\n",
    "- **RxRelease / Haru2.4.0dev**: A less common producer, possibly indicating another application or platform for PDF creation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Leemos el archivo CSV\n",
    "df = pd.read_csv(r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\ETF Layer2 - Pdf_datset\\metadata_2.0.csv')\n",
    "\n",
    "# Contamos la cantidad de archivos por productor\n",
    "conteo_producers = df['Producer'].value_counts()\n",
    "\n",
    "# Mostramos el resultado\n",
    "print(conteo_producers)\n",
    "\n",
    "# Visualizamos el resultado en un gráfico de barras\n",
    "conteo_producers.plot(kind='bar', figsize=(10,6))\n",
    "plt.title('Cantidad de Archivos por Productor')\n",
    "plt.xlabel('Productor')\n",
    "plt.ylabel('Cantidad de Archivos')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FileSize:\n",
    "\n",
    "- Most of the PDFs have 2-3 pages. However, there are some with more pages, up to 11 pages in one case. We studied the distribution of the number of pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\ETF Layer2 - Pdf_datset\\metadata_2.0.csv')\n",
    "\n",
    "# Count the number of files per number of pages\n",
    "page_count = df['TotalPages'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(page_count)\n",
    "\n",
    "# Visualize the result in a bar plot\n",
    "page_count.plot(kind='bar', figsize=(10,6))\n",
    "plt.title('Files by Number of Pages')\n",
    "plt.xlabel('Pages')\n",
    "plt.ylabel('Number of Files')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TotalPages:\n",
    "\n",
    "- File sizes vary widely, ranging from just over 100KB to over 13MB, indicating a diversity in the content of these documents (e.g., plain text vs high-resolution images).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\ETF Layer2 - Pdf_datset\\metadata_2.0.csv') \n",
    "# Convert sizes from B to MB\n",
    "df['FileSize'] = df['FileSize'] / 1024 / 1024\n",
    "\n",
    "# We can get some basic statistics\n",
    "statistics_size = df['FileSize'].describe()\n",
    "print(statistics_size)\n",
    "\n",
    "# Histogram to visualize the distribution of file sizes\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(df['FileSize'], bins=50, color='blue', edgecolor='black')\n",
    "plt.title('File Size Distribution')\n",
    "plt.xlabel('File Size (MB)')\n",
    "plt.ylabel('Number of Files')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "data = route\n",
    "\n",
    "# Convert sizes from B to MB\n",
    "data['FileSize'] = data['FileSize'] / 1024 / 1024\n",
    "\n",
    "# Group by 'Producer' and calculate the total sum of 'FileSize' for each 'Producer'\n",
    "grouped_data = data.groupby('Producer')['FileSize'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "grouped_data.plot(kind='barh', color='skyblue')\n",
    "plt.title('Total File Size by Producer')\n",
    "plt.xlabel('Total Size (unknown units)')\n",
    "plt.ylabel('Producer')\n",
    "plt.gca().invert_yaxis()  # This is to have the producer with the largest size at the top\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "route = r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\ETF Layer2 - Pdf_datset\\metadata_2.0.csv'\n",
    "\n",
    "# Read the CSV file and load it into a DataFrame\n",
    "data = pd.read_csv(route)\n",
    "\n",
    "# Create a dictionary that assigns a unique code to each producer\n",
    "producer_codes = {producer: code for code, producer in enumerate(data['Producer'].unique(), start=1)}\n",
    "\n",
    "# Map the dictionary to the DataFrame to create the new column\n",
    "data['producer_code'] = data['Producer'].map(producer_codes)\n",
    "\n",
    "# Create a reference DataFrame for the producer codes\n",
    "producer_reference = pd.DataFrame(list(producer_codes.items()), columns=['Producer', 'producer_code'])\n",
    "\n",
    "# Save the reference DataFrame to another CSV\n",
    "reference_route = r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\ETF Layer2 - Pdf_datset\\producer_reference.csv'\n",
    "producer_reference.to_csv(reference_route, index=False)\n",
    "\n",
    "# Save the modified DataFrame back to the original CSV\n",
    "data.to_csv(route, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the CSV file\n",
    "route = r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\ETF Layer2 - Pdf_datset\\metadata_2.0.csv'\n",
    "\n",
    "# Read the CSV file and load it into a DataFrame\n",
    "data = pd.read_csv(route)\n",
    "\n",
    "# Convert FileSize from Bytes to Megabytes (MB)\n",
    "data['FileSize'] = data['FileSize'] / (1024 * 1024)\n",
    "\n",
    "# Visualize all relationships between pairs of variables\n",
    "# Use the 'hue' argument to color the points according to the 'Producer'\n",
    "sns.pairplot(data, hue='Producer', diag_kind=\"kde\", markers='o', plot_kws={'alpha': 0.9}, height=2.5)\n",
    "plt.suptitle('Relationships between pairs of variables', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title & Author:\n",
    "\n",
    "- In several cases, the title and author fields appear to contain names, suggesting that these documents are related to individual persons. Some titles such as \"Screenshot 08-01-2022 13.59\" indicate that they might have been taken as captures or scans of other content.\n",
    "\n",
    "### CreationDate & ModDate:\n",
    "- The documents span a range of creation and modification dates, with the most recent ones from January 2023 and the oldest from February 2022.\n",
    "\n",
    "### Subject:\n",
    "- Some documents have specific subjects mentioned, but they do not contain relevant information.\n",
    "\n",
    "### Null data:\n",
    "- Not all fields are filled for each entry, but it is not a significant consideration in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Comparison of Low-Quality Files\n",
    "\n",
    "We have identified a specific set of files that exhibit different characteristics from the rest of our dataset. These files, listed below, are images that have been converted to PDFs.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "These files inherently display lower quality compared to others due to their origin as images. This could affect the accuracy and performance of machine learning models if mixed with higher-quality files.\n",
    "\n",
    "Hence, our goal is to establish a procedure that allows us to effectively segment these files to consider the possibility of training two different models: one for converted image files and another for the rest.\n",
    "\n",
    "- List of Files Converted from Images to PDFs\n",
    "\n",
    "    - **Form1965 ENF**\n",
    "    - **Form1973 ENF**\n",
    "    - **Form16 MED**\n",
    "    - **Form19 CUI**\n",
    "    - **Form26 MED**\n",
    "    - **Form31 CUI**\n",
    "    - **Form100 CUI**\n",
    "    - **Form101 ENF**\n",
    "    - **Form103 ENF**\n",
    "    - **Form251 MED**\n",
    "    - **Form261 CUI**\n",
    "    - **Form487 ENF**\n",
    "    - **Form1302 ENF**\n",
    "\n",
    "- Next Steps\n",
    "\n",
    "1. Segment these files from the main dataset.\n",
    "2. Analyze the characteristics and quality of these files.\n",
    "3. Decide on the training methodology and modeling strategies for this data.\n",
    "\n",
    "**Note:** It is essential to adopt a systematic and data-driven approach for this process, ensuring that any decision made benefits the quality and accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "def clean_directory_name(name):\n",
    "    # Replace forbidden characters\n",
    "    name = re.sub(r'[<>:\"/\\\\|?*]', '_', name)\n",
    "    # Remove any trailing dot\n",
    "    name = name.rstrip('.')\n",
    "    return name\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_route = r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\ETF Layer2 - Pdf_datset\\metadata_2.0.csv'\n",
    "\n",
    "# Path of the folder containing all the files\n",
    "source_folder = r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\ETF Layer2 - Pdf_datset\\Training Dataset\\metadata_sample' \n",
    "\n",
    "# Read the CSV file and load it into a DataFrame\n",
    "data = pd.read_csv(csv_route)\n",
    "\n",
    "# Iterate over each row to obtain the producer and the file name\n",
    "for index, row in data.iterrows():\n",
    "    producer = clean_directory_name(row['Producer'])\n",
    "    file_name = row['Name']  \n",
    "\n",
    "    # Create a folder for the producer if it does not exist yet\n",
    "    producer_folder = os.path.join(source_folder, producer)\n",
    "    if not os.path.exists(producer_folder):\n",
    "        os.makedirs(producer_folder)\n",
    "\n",
    "    # Move the file to the producer's folder\n",
    "    source_file_path = os.path.join(source_folder, file_name)\n",
    "    destination_file_path = os.path.join(producer_folder, file_name)\n",
    "\n",
    "    # Check if the file exists and then move it\n",
    "    if os.path.exists(source_file_path):\n",
    "        shutil.move(source_file_path, destination_file_path)\n",
    "    else:\n",
    "        print(f\"The file {file_name} was not found in {source_folder}\")\n",
    "\n",
    "print(\"Files organized according to producers.\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low-quality files are located to determine a potential segmentation process for them, and a report is prepared with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List of files to search for\n",
    "files_to_search = [\n",
    "    \"Form1965- ENF.pdf\",\n",
    "    \"Form1973- ENF.pdf\",\n",
    "    \"Form16- MED.pdf\",\n",
    "    \"Form19- CUI.pdf\",\n",
    "    \"Form26- MED.pdf\",\n",
    "    \"Form31- CUI.pdf\",\n",
    "    \"Form100- CUI.pdf\",\n",
    "    \"Form101- ENF.pdf\",\n",
    "    \"Form103- ENF.pdf\",\n",
    "    \"Form251- MED.pdf\",\n",
    "    \"Form261- CUI.pdf\",\n",
    "    \"Form487- ENF.pdf\",\n",
    "    \"Form1302- ENF.pdf\"\n",
    "]\n",
    "\n",
    "# Root directory to search for the files\n",
    "route = r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\Layer2 - Pdf_dataset'\n",
    "root_folder = os.path.join(route, 'metadata_sample')\n",
    "\n",
    "# Report file\n",
    "report_file_path = os.path.join(route, 'metadata_report.txt')\n",
    "\n",
    "# Open the report file in write mode\n",
    "with open(report_file_path, 'w') as report_file:\n",
    "    # Traverse through folders and subfolders within the root directory\n",
    "    for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "        for file_name in filenames:\n",
    "            # If the file name is in the list of files to search for\n",
    "            if file_name in files_to_search:\n",
    "                report_file.write(f\"File {file_name} found at: {folder_name}\\n\")\n",
    "\n",
    "print(f\"Report saved at {report_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: It is decided to use a file segmentation model based on the data source. Since high-quality files are generated by the same source, Samsung-M4580FX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretations and Considerations\n",
    "\n",
    "1. The **Samsung-M4580FX** brand appears to be the dominant producer and is also linked to the main creator. This could be important in future analysis, especially when considering the quality or specific characteristics of files from this source.\n",
    "\n",
    "2. The lack of data in the 'Creator' column is not a cause for concern as it has been determined through investigation that the creator is the Samsung-M4580FX machine.\n",
    "\n",
    "3. The process of separating data continues with the purpose of separating files into high and low-quality formats for model training.\n",
    "\n",
    "- **High Quality Data:** Files generated by `Samsung-M4580FX`\n",
    "- **Low Quality Data:** Files generated by other machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pixel Area Analysis\n",
    "\n",
    "Based on the requirements of GCP Document AI - Form Recognizer, it is determined that PDF files must have a size smaller than 10000 x 10000 pixels, with a resolution of 150 dpi.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "- The size of the pages is analyzed to determine if they meet the requirements of GCP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 1 PDF to check if each page measures less than 10000x10000 pixels\n",
    "import PyPDF2\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def analyze_pdf(file_path):\n",
    "    # Open the PDF file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Use PyPDF2 to determine the number of pages\n",
    "        pdf = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(pdf.pages)\n",
    "        \n",
    "        # Convert PDF pages to images\n",
    "        images = convert_from_path(file_path)\n",
    "\n",
    "        # Store the number of pixels for each page\n",
    "        pixels_per_page = []\n",
    "        \n",
    "        for image in images:\n",
    "            width, height = image.size\n",
    "            pixels_per_page.append(width * height)\n",
    "        \n",
    "        return pixels_per_page\n",
    "\n",
    "file_path = r'C:\\Users\\HP\\My Drive\\Inteligencia Artificial\\PROJECTS\\ML Ops - House24 - Form Recognizer\\Layer2 - Pdf_dataset\\metadata_sample\\Samsung-M4580FX\\Form256- FONO.pdf'  # Replace this with your file path\n",
    "pixels_per_page = analyze_pdf(file_path)\n",
    "\n",
    "for i, pixels in enumerate(pixels_per_page, 1):\n",
    "    print(f\"Page {i}: {pixels} pixels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "- It is determined that the files meet the requirements of GCP Document AI, as they have a size smaller than 10000 x 10000 pixels, with a resolution of 150 dpi.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
